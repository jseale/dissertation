{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Trains and evaluates DistilBert token classification model with variant-only Wikipedia Homograph data only; one model for all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Tuple, TextIO\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertForTokenClassification, DistilBertForTokenClassification\n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast, EvalPrediction\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertConfig\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "#from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#from pytorch_lightning.metrics.functional.classification import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Paths \n",
    "WHD_BERT_DATA = \"C:/Users/jseal/Dev/dissertation/Data/WHD_Bert/variant_only/\"\n",
    "WHD = 'C:/Users/jseal/Dev/dissertation/Data/WikipediaHomographData/data/'\n",
    "\n",
    "#Txt splits\n",
    "TRAIN_TXT = WHD_BERT_DATA + \"train.txt\"\n",
    "DEV_TXT = WHD_BERT_DATA + \"dev.txt\"\n",
    "TEST_TXT = WHD_BERT_DATA + \"test.txt\"\n",
    "\n",
    "#Logging path\n",
    "LOGS = \"./whd_variant_only_logs\"\n",
    "\n",
    "#Labels file\n",
    "LABELS = WHD_BERT_DATA + \"labels.txt\"\n",
    "\n",
    "#Homographs\n",
    "#whd_df = pd.read_csv(WHD + 'WikipediaHomographData.csv')\n",
    "#homographs = whd_df.drop_duplicates(subset='homograph')['homograph'].tolist()\n",
    "\n",
    "# Model Variables\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "BATCH_SIZE = 16 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED_1 = 15\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "STEPS = \"steps\"\n",
    "EVAL_STEPS = 100\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "class WHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "       # self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def align_predictions(predictions: np.ndarray, \n",
    "                      label_ids: np.ndarray) -> Tuple[List[int], List[int], List[str]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                preds_list[i].append(id2tag[preds[i][j]])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def homograph_check(preds_list, out_label_list):\n",
    "    '''Check that predictions to be evaluated are homographs, \n",
    "    not tokens from the sentence provided as context.'''\n",
    "    pred_labels = zip(preds_list, out_label_list)\n",
    "    unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for e in pred_labels:\n",
    "        pred_label_pair = zip(e[0], e[1])\n",
    "        for pred, label in pred_label_pair:\n",
    "            if label is not 'O':\n",
    "                preds.append(pred)\n",
    "                labels.append(label)\n",
    "    return labels, preds\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    '''Only compute metrics on homograph pronunciation predictions'''\n",
    "    preds_list, labels_list = align_predictions(p.predictions, p.label_ids)\n",
    "    targets, preds = homograph_check(preds_list, labels_list)\n",
    "#     preds = [tag2id[pred] for pred in preds]\n",
    "#     targets = [tag2id[target] for target in targets]\n",
    "#     preds = torch.as_tensor(preds)\n",
    "#     targets = torch.as_tensor(targets)\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(targets, preds),\n",
    "        \"balanced_accuracy_score\": balanced_accuracy_score(targets, preds),\n",
    "        #\"micro_auc_roc\": roc_auc_score(targets, preds, average='micro')\n",
    "#         \"micro_accuracy\": accuracy(preds, targets, class_reduction='micro', num_classes=len(unique_tags)),\n",
    "#         \"macro_accuracy\": accuracy(preds, targets, class_reduction='macro', num_classes=len(unique_tags)),\n",
    "    }\n",
    "\n",
    "def write_predictions_to_file(writer: TextIO, test_input_reader: TextIO, preds_list: List):\n",
    "    example_id = 0\n",
    "    for line in test_input_reader:\n",
    "        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "            writer.write(line)\n",
    "            if not preds_list[example_id]:\n",
    "                example_id += 1\n",
    "        elif preds_list[example_id]:\n",
    "            output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "            writer.write(output_line)\n",
    "        else:\n",
    "            logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique labels, enumeration/IDs, and make mappings\n",
    "unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make separate lists of aligned texts and tags\n",
    "train_texts, train_tags = read_set(TRAIN_TXT)\n",
    "dev_texts, dev_tags = read_set(DEV_TXT)\n",
    "test_texts, test_tags = read_set(TEST_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for text \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "dev_encodings = tokenizer(dev_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for labels\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "dev_labels = encode_tags(dev_tags, dev_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets\n",
    "train_encodings.pop(\"offset_mapping\") # don't want to pass this to the model\n",
    "dev_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WHDataset(train_encodings, train_labels)\n",
    "dev_dataset = WHDataset(dev_encodings, dev_labels)\n",
    "#print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2172' max='2172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2172/2172 11:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Balanced Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.443677</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.002966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302041</td>\n",
       "      <td>0.169324</td>\n",
       "      <td>0.126098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.160077</td>\n",
       "      <td>0.683371</td>\n",
       "      <td>0.512469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.041314</td>\n",
       "      <td>0.094725</td>\n",
       "      <td>0.809415</td>\n",
       "      <td>0.609677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.041314</td>\n",
       "      <td>0.074272</td>\n",
       "      <td>0.813212</td>\n",
       "      <td>0.619394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.041314</td>\n",
       "      <td>0.057715</td>\n",
       "      <td>0.851177</td>\n",
       "      <td>0.681655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.041314</td>\n",
       "      <td>0.052382</td>\n",
       "      <td>0.873197</td>\n",
       "      <td>0.706596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.041314</td>\n",
       "      <td>0.045046</td>\n",
       "      <td>0.886105</td>\n",
       "      <td>0.744249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.044467</td>\n",
       "      <td>0.878512</td>\n",
       "      <td>0.744477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.037266</td>\n",
       "      <td>0.913440</td>\n",
       "      <td>0.789840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.033827</td>\n",
       "      <td>0.924829</td>\n",
       "      <td>0.808538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.032919</td>\n",
       "      <td>0.927866</td>\n",
       "      <td>0.828576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.921792</td>\n",
       "      <td>0.817315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.029373</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.852048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.028632</td>\n",
       "      <td>0.940774</td>\n",
       "      <td>0.863632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.941534</td>\n",
       "      <td>0.862112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.026254</td>\n",
       "      <td>0.938497</td>\n",
       "      <td>0.857386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.026963</td>\n",
       "      <td>0.949127</td>\n",
       "      <td>0.876954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.026710</td>\n",
       "      <td>0.952164</td>\n",
       "      <td>0.882881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.025256</td>\n",
       "      <td>0.948368</td>\n",
       "      <td>0.875493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='162' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.025255737826228142,\n",
       " 'eval_accuracy_score': 0.9483675018982536,\n",
       " 'eval_balanced_accuracy_score': 0.8754930766632892,\n",
       " 'epoch': 3.0,\n",
       " 'total_flos': 1132382659351500}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "# config = DistilBertConfig.from_pretrained(MODEL_NAME, \n",
    "#                                           output_hidden_states=True, \n",
    "#                                           #idx2label=id2tag, \n",
    "#                                           num_labels=len(unique_tags))\n",
    "# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\n",
    "model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                         num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-only-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_1,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Accuracy\n",
      "0.9483675018982536\n",
      "Macro Accuracy\n",
      "0.8754930766632895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\transformers\\trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
      "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "predictions, label_ids, metrics = trainer.predict(dev_dataset)\n",
    "preds_list, targets_list = align_predictions(predictions, label_ids)\n",
    "targets_list, preds_list = homograph_check(preds_list, targets_list)\n",
    "\n",
    "output_results_file = os.path.join(training_args.output_dir, \"results.txt\")\n",
    "if trainer.is_world_master():\n",
    "    with open(output_results_file, \"w\", encoding=\"utf8\") as writer:\n",
    "        for key, value in metrics.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "# Save predictions\n",
    "output_predictions_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "to_df = zip(preds_list, targets_list)\n",
    "print('Micro Accuracy')\n",
    "df = pd.DataFrame(to_df, columns=['Preds', 'Targets'])\n",
    "df_correct = df[df['Preds'] == df['Targets']]\n",
    "print(len(df_correct)/len(df))\n",
    "print('Macro Accuracy')\n",
    "group_accuracies = []\n",
    "for idx, group in df.groupby('Targets'): \n",
    "    correct = group[group['Preds'] == group['Targets']]\n",
    "    accuracy = len(correct)/len(group)\n",
    "    group_accuracies.append(accuracy)\n",
    "print(mean(group_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_huggingface",
   "language": "python",
   "name": "dissertation_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
