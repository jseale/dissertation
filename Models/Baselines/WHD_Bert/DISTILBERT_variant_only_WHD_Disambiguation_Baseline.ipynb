{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Trains and evaluates DistilBert token classification model with variant-only Wikipedia Homograph data only; one model for all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59fa8d61393d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForTokenClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Tuple, TextIO\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertForTokenClassification, DistilBertForTokenClassification\n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast, EvalPrediction\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertConfig\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "#from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#from pytorch_lightning.metrics.functional.classification import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths \n",
    "WHD_BERT_DATA = \"C:/Users/jseal/Dev/dissertation/Data/WHD_Bert/variant_only/\"\n",
    "WHD = 'C:/Users/jseal/Dev/dissertation/Data/WikipediaHomographData/data/'\n",
    "\n",
    "#Txt splits\n",
    "TRAIN_TXT = WHD_BERT_DATA + \"train.txt\"\n",
    "DEV_TXT = WHD_BERT_DATA + \"dev.txt\"\n",
    "TEST_TXT = WHD_BERT_DATA + \"test.txt\"\n",
    "\n",
    "#Logging path\n",
    "LOGS = \"./whd_variant_only_logs\"\n",
    "\n",
    "#Labels file\n",
    "LABELS = WHD_BERT_DATA + \"labels.txt\"\n",
    "\n",
    "#Homographs\n",
    "#whd_df = pd.read_csv(WHD + 'WikipediaHomographData.csv')\n",
    "#homographs = whd_df.drop_duplicates(subset='homograph')['homograph'].tolist()\n",
    "\n",
    "# Model Variables\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "BATCH_SIZE = 16 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED_1 = 15\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "STEPS = \"steps\"\n",
    "EVAL_STEPS = 100\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "class WHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "       # self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def align_predictions(predictions: np.ndarray, \n",
    "                      label_ids: np.ndarray) -> Tuple[List[int], List[int], List[str]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                preds_list[i].append(id2tag[preds[i][j]])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def homograph_check(preds_list, out_label_list):\n",
    "    '''Check that predictions to be evaluated are homographs, \n",
    "    not tokens from the sentence provided as context.'''\n",
    "    pred_labels = zip(preds_list, out_label_list)\n",
    "    unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for e in pred_labels:\n",
    "        pred_label_pair = zip(e[0], e[1])\n",
    "        for pred, label in pred_label_pair:\n",
    "            if label is not 'O':\n",
    "                preds.append(pred)\n",
    "                labels.append(label)\n",
    "    return labels, preds\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    '''Only compute metrics on homograph pronunciation predictions'''\n",
    "    preds_list, labels_list = align_predictions(p.predictions, p.label_ids)\n",
    "    targets, preds = homograph_check(preds_list, labels_list)\n",
    "#     preds = [tag2id[pred] for pred in preds]\n",
    "#     targets = [tag2id[target] for target in targets]\n",
    "#     preds = torch.as_tensor(preds)\n",
    "#     targets = torch.as_tensor(targets)\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(targets, preds),\n",
    "        \"balanced_accuracy_score\": balanced_accuracy_score(targets, preds),\n",
    "        #\"micro_auc_roc\": roc_auc_score(targets, preds, average='micro')\n",
    "#         \"micro_accuracy\": accuracy(preds, targets, class_reduction='micro', num_classes=len(unique_tags)),\n",
    "#         \"macro_accuracy\": accuracy(preds, targets, class_reduction='macro', num_classes=len(unique_tags)),\n",
    "    }\n",
    "\n",
    "def write_predictions_to_file(writer: TextIO, test_input_reader: TextIO, preds_list: List):\n",
    "    example_id = 0\n",
    "    for line in test_input_reader:\n",
    "        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "            writer.write(line)\n",
    "            if not preds_list[example_id]:\n",
    "                example_id += 1\n",
    "        elif preds_list[example_id]:\n",
    "            output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "            writer.write(output_line)\n",
    "        else:\n",
    "            logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique labels, enumeration/IDs, and make mappings\n",
    "unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make separate lists of aligned texts and tags\n",
    "train_texts, train_tags = read_set(TRAIN_TXT)\n",
    "dev_texts, dev_tags = read_set(DEV_TXT)\n",
    "test_texts, test_tags = read_set(TEST_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for text \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "dev_encodings = tokenizer(dev_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for labels\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "dev_labels = encode_tags(dev_tags, dev_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets\n",
    "train_encodings.pop(\"offset_mapping\") # don't want to pass this to the model\n",
    "dev_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WHDataset(train_encodings, train_labels)\n",
    "dev_dataset = WHDataset(dev_encodings, dev_labels)\n",
    "#print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "# config = DistilBertConfig.from_pretrained(MODEL_NAME, \n",
    "#                                           output_hidden_states=True, \n",
    "#                                           #idx2label=id2tag, \n",
    "#                                           num_labels=len(unique_tags))\n",
    "# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\n",
    "model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                         num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-only-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_1,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "predictions, label_ids, metrics = trainer.predict(dev_dataset)\n",
    "preds_list, targets_list = align_predictions(predictions, label_ids)\n",
    "targets_list, preds_list = homograph_check(preds_list, targets_list)\n",
    "\n",
    "output_results_file = os.path.join(training_args.output_dir, \"results.txt\")\n",
    "if trainer.is_world_master():\n",
    "    with open(output_results_file, \"w\", encoding=\"utf8\") as writer:\n",
    "        for key, value in metrics.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "# Save predictions\n",
    "#output_predictions_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "to_df = zip(preds_list, targets_list)\n",
    "print('Micro Accuracy')\n",
    "df = pd.DataFrame(to_df, columns=['Preds', 'Targets'])\n",
    "df_correct = df[df['Preds'] == df['Targets']]\n",
    "print(len(df_correct)/len(df))\n",
    "print('Macro Accuracy')\n",
    "group_accuracies_for_mean = []\n",
    "group_accuracies = []\n",
    "for idx, group in df.groupby('Targets'):\n",
    "    grp = {}\n",
    "    grp[\"wordid\"] = idx\n",
    "    print(idx)\n",
    "    correct = group[group['Preds'] == group['Targets']]\n",
    "    accuracy = len(correct)/len(group)\n",
    "    print(accuracy)\n",
    "    grp[\"accuracy\"] = accuracy\n",
    "    group_accuracies.append(grp)\n",
    "    group_accuracies_for_mean.append(accuracy)\n",
    "print(mean(group_accuracies_for_mean))\n",
    "accuracies_df = pd.DataFrame(group_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions_file = os.path.join('./whd-variant-only-model', \"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df.sort_values(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids = pd.read_csv(\"C:/Users/jseal/Dev/dissertation/Data_Exploration/WHD/hids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc.sort_values([\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = hids_acc[hids_acc['accuracy'] < 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(15,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "homs = hids_acc[hids_acc[\"percent_ttl\"] < 0.2]['homograph'].tolist()\n",
    "homs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(homs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc['homograph'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and eval together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids = pd.read_csv(\"C:/Users/jseal/Dev/dissertation/Data_Exploration/WHD/train_eval_hids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "hids_acc = hids_acc[hids_acc['accuracy'] < 1.0]\n",
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(20,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "hids_acc = hids_acc[hids_acc['percent_ttl'] < 0.1]\n",
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(20,10))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "dissertation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
