{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Trains and evaluates DistilBert token classification model with Wikipedia Homograph data only; one model for all labels. To Do: Finish metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertForTokenClassification, DistilBertForTokenClassification\n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast, EvalPrediction\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Paths \n",
    "WHD_BERT_DATA = \"C:/Users/jseal/Dev/dissertation/Data/WHD_Bert/\" \n",
    "#Txt splits\n",
    "TRAIN_TXT = WHD_BERT_DATA + \"train.txt\"\n",
    "DEV_TXT = WHD_BERT_DATA + \"dev.txt\"\n",
    "TEST_TXT = WHD_BERT_DATA + \"test.txt\"\n",
    "#Logging path\n",
    "LOGS = \"./logs\"\n",
    "#Labels file\n",
    "LABELS = WHD_BERT_DATA + \"labels.txt\"\n",
    "\n",
    "# Model Variables\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "OUTPUT_DIR = WHD_BERT_DATA + \"whd-model/\" \n",
    "BATCH_SIZE = 16 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "STEPS = \"steps\"\n",
    "EVAL_STEPS = 100\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "class WHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                preds_list[i].append(id2tag[preds[i][j]])\n",
    "\n",
    "    return preds_list, out_label_list\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique labels, enumeration/IDs, and make mappings\n",
    "unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make separate lists of aligned texts and tags\n",
    "train_texts, train_tags = read_set(TRAIN_TXT)\n",
    "dev_texts, dev_tags = read_set(DEV_TXT)\n",
    "test_texts, test_tags = read_set(TEST_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for text \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "dev_encodings = tokenizer(dev_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for labels\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "dev_labels = encode_tags(dev_tags, dev_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets\n",
    "train_encodings.pop(\"offset_mapping\") # don't want to pass this to the model\n",
    "dev_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WHDataset(train_encodings, train_labels)\n",
    "dev_dataset = WHDataset(dev_encodings, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2445' max='2445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2445/2445 05:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620508</td>\n",
       "      <td>0.934024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463237</td>\n",
       "      <td>0.934024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.328569</td>\n",
       "      <td>0.946250</td>\n",
       "      <td>0.678005</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.310972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190631</td>\n",
       "      <td>0.975148</td>\n",
       "      <td>0.811289</td>\n",
       "      <td>0.649798</td>\n",
       "      <td>0.721619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.104156</td>\n",
       "      <td>0.983462</td>\n",
       "      <td>0.815826</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.800687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.073575</td>\n",
       "      <td>0.985951</td>\n",
       "      <td>0.824645</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.823251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.061612</td>\n",
       "      <td>0.987818</td>\n",
       "      <td>0.839841</td>\n",
       "      <td>0.856275</td>\n",
       "      <td>0.847979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>0.988396</td>\n",
       "      <td>0.858784</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>0.858204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.042817</td>\n",
       "      <td>0.048423</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>0.876749</td>\n",
       "      <td>0.887989</td>\n",
       "      <td>0.882333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.991019</td>\n",
       "      <td>0.886980</td>\n",
       "      <td>0.905533</td>\n",
       "      <td>0.896160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.991864</td>\n",
       "      <td>0.900534</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.905369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>0.992575</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.916329</td>\n",
       "      <td>0.917878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>0.993509</td>\n",
       "      <td>0.924110</td>\n",
       "      <td>0.928475</td>\n",
       "      <td>0.926287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>0.993420</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.929198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.993998</td>\n",
       "      <td>0.930574</td>\n",
       "      <td>0.940621</td>\n",
       "      <td>0.935570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.028539</td>\n",
       "      <td>0.994176</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.943320</td>\n",
       "      <td>0.937626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.029364</td>\n",
       "      <td>0.993954</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.936937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.027791</td>\n",
       "      <td>0.994576</td>\n",
       "      <td>0.941414</td>\n",
       "      <td>0.943320</td>\n",
       "      <td>0.942366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.994665</td>\n",
       "      <td>0.940309</td>\n",
       "      <td>0.946019</td>\n",
       "      <td>0.943155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.994887</td>\n",
       "      <td>0.944892</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.946801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.024849</td>\n",
       "      <td>0.995376</td>\n",
       "      <td>0.950235</td>\n",
       "      <td>0.953441</td>\n",
       "      <td>0.951836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.024505</td>\n",
       "      <td>0.995110</td>\n",
       "      <td>0.945077</td>\n",
       "      <td>0.952092</td>\n",
       "      <td>0.948571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.995154</td>\n",
       "      <td>0.943963</td>\n",
       "      <td>0.954791</td>\n",
       "      <td>0.949346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.995154</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.953441</td>\n",
       "      <td>0.949597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.024505170062184334,\n",
       " 'eval_accuracy_score': 0.9951095896501133,\n",
       " 'eval_precision': 0.9450770261219023,\n",
       " 'eval_recall': 0.9520917678812416,\n",
       " 'eval_f1': 0.9485714285714286,\n",
       " 'epoch': 3.0,\n",
       " 'total_flos': 1274150476314000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_huggingface",
   "language": "python",
   "name": "dissertation_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
