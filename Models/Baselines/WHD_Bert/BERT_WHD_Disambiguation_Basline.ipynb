{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertForTokenClassification, DistilBertForTokenClassification\n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Paths \n",
    "WHD_BERT_DATA = \"C:/Users/jseal/Dev/dissertation/Data/WikipediaHomographData/data/bert_data/\" \n",
    "BERT_TRAIN = WHD_BERT_DATA + \"train/\"\n",
    "BERT_VAL = WHD_BERT_DATA + \"valid/\"\n",
    "BERT_TEST = WHD_BERT_DATA + \"test/\"\n",
    "ALL_SPLITS = [BERT_TRAIN,BERT_VAL,BERT_TEST]\n",
    "\n",
    "TRAIN_TMP = WHD_BERT_DATA + \"train.txt.tmp\"\n",
    "VAL_TMP = WHD_BERT_DATA + \"val.txt.tmp\"\n",
    "TEST_TMP = WHD_BERT_DATA + \"test.txt.tmp\"\n",
    "TMPS = [TRAIN_TMP, VAL_TMP, TEST_TMP]\n",
    "\n",
    "TRAIN_TXT = WHD_BERT_DATA + \"train.txt\"\n",
    "VAL_TXT = WHD_BERT_DATA + \"val.txt\"\n",
    "TEST_TXT = WHD_BERT_DATA + \"test.txt\"\n",
    "OUTS = [TRAIN_TXT, VAL_TXT, TEST_TXT]\n",
    "\n",
    "TMPS_OUTS = zip(TMPS, OUTS)\n",
    "\n",
    "NER = \"C:/Users/jseal/Dev/dissertation/transformers/examples/token-classification/\"\n",
    "RUN_NER = NER + \"run_ner.py\"\n",
    "\n",
    "PREPROCESS = NER + \"scripts/preprocess.py\"\n",
    "\n",
    "#Labels file\n",
    "LABELS = WHD_BERT_DATA + \"labels.txt\"\n",
    "\n",
    "# Model Variables\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "OUTPUT_DIR = \"C:/Users/jseal/Dev/dissertation/Models/Baselines/WHD_Bert/whd-model\" #@param [\"spanberta-ner\", \"bert-base-ml-ner\"]\n",
    "BATCH_SIZE = 16 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Preprocessing\n",
    "# # Write temporary train, val, and test txt files\n",
    "# for tmp in TMPS:\n",
    "#     for split_path in ALL_SPLITS:\n",
    "#         with open(tmp, 'w', encoding=\"utf8\") as f_out: \n",
    "#             for f in glob(split_path + \"*\"):\n",
    "#                 with open(f, encoding=\"utf8\") as example:\n",
    "#                     lines = example.readlines()\n",
    "#                     for line in lines: \n",
    "#                         line_list = line.split('\\t')\n",
    "#                         f_out.write(line_list[1] + '\\t' + line_list[2])\n",
    "#                 f_out.write('\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword_len_counter = 0\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# MAX_LENGTH -= tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "# for tmp, outfile in TMPS_OUTS:\n",
    "#     with open(tmp, \"r\", encoding=\"utf8\") as f_p:\n",
    "#         with open(outfile, \"w\", encoding=\"utf8\") as out_f: \n",
    "#             for line in f_p:\n",
    "#                 line = line.rstrip()\n",
    "\n",
    "#                 if not line:\n",
    "#                     out_f.write(line +\"\\n\")\n",
    "#                     subword_len_counter = 0\n",
    "#                     continue\n",
    "\n",
    "#                 token = line.split()[0]\n",
    "\n",
    "#                 current_subwords_len = len(tokenizer.tokenize(token))\n",
    "\n",
    "#                 # Token contains strange control characters like \\x96 or \\x95\n",
    "#                 # Just filter out the complete line\n",
    "#                 if current_subwords_len == 0:\n",
    "#                     continue\n",
    "\n",
    "#                 if (subword_len_counter + current_subwords_len) > MAX_LENGTH:\n",
    "#                     out_f.write(\"\\n\")\n",
    "#                     out_f.write(line +\"\\n\")\n",
    "#                     subword_len_counter = current_subwords_len\n",
    "#                     continue\n",
    "\n",
    "#                 subword_len_counter += current_subwords_len\n",
    "\n",
    "#                 out_f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_tags = read_set(TRAIN_TXT)\n",
    "val_texts, val_tags = read_set(VAL_TXT)\n",
    "test_texts, test_tags = read_set(TEST_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "#val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WHDataset(train_encodings, train_labels)\n",
    "val_dataset = WHDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 3.315\n",
      "[1,    20] loss: 0.660\n",
      "[1,    30] loss: 0.637\n",
      "[1,    40] loss: 0.644\n",
      "[1,    50] loss: 0.635\n",
      "[1,    60] loss: 0.616\n",
      "[1,    70] loss: 0.625\n",
      "[1,    80] loss: 0.618\n",
      "[1,    90] loss: 0.592\n",
      "[1,   100] loss: 0.617\n",
      "[2,    10] loss: 0.574\n",
      "[2,    20] loss: 0.571\n",
      "[2,    30] loss: 0.538\n",
      "[2,    40] loss: 0.517\n"
     ]
    }
   ],
   "source": [
    "#Model & tokenizer\n",
    "model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_huggingface",
   "language": "python",
   "name": "dissertation_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
