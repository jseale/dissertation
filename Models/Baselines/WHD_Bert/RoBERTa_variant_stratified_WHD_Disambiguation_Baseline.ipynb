{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Trains and evaluates DistilBert token classification model with variant-only Wikipedia Homograph data only; one model for all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Tuple, TextIO\n",
    "from transformers import RobertaForTokenClassification\n",
    "from transformers import RobertaTokenizerFast, EvalPrediction\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "#from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#from pytorch_lightning.metrics.functional.classification import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Paths \n",
    "WHD_BERT_DATA = \"C:/Users/jseal/Dev/dissertation/Data/WHD_RoBERTa/variant_stratified/\"\n",
    "WHD = 'C:/Users/jseal/Dev/dissertation/Data/WikipediaHomographData/data/'\n",
    "\n",
    "#Txt splits\n",
    "TRAIN_TXT = WHD_BERT_DATA + \"train.txt\"\n",
    "DEV_TXT = WHD_BERT_DATA + \"dev.txt\"\n",
    "TEST_TXT = WHD_BERT_DATA + \"test.txt\"\n",
    "\n",
    "#Logging path\n",
    "LOGS = './whd-variant-stratified-roberta-model_logs'\n",
    "\n",
    "#Labels file\n",
    "LABELS = WHD_BERT_DATA + \"labels.txt\"\n",
    "\n",
    "#Homographs\n",
    "#whd_df = pd.read_csv(WHD + 'WikipediaHomographData.csv')\n",
    "#homographs = whd_df.drop_duplicates(subset='homograph')['homograph'].tolist()\n",
    "\n",
    "# Model Variables\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "BATCH_SIZE = 16 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED_1 = 15\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "SEED_2 = 30\n",
    "SEED_3 = 1234\n",
    "SEED_4 = 500\n",
    "STEPS = \"steps\"\n",
    "EVAL_STEPS = 100\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "class WHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "       # self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def align_predictions(predictions: np.ndarray, \n",
    "                      label_ids: np.ndarray) -> Tuple[List[int], List[int], List[str]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                preds_list[i].append(id2tag[preds[i][j]])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def homograph_check(preds_list, out_label_list):\n",
    "    '''Check that predictions to be evaluated are homographs, \n",
    "    not tokens from the sentence provided as context.'''\n",
    "    pred_labels = zip(preds_list, out_label_list)\n",
    "    unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for e in pred_labels:\n",
    "        pred_label_pair = zip(e[0], e[1])\n",
    "        for pred, label in pred_label_pair:\n",
    "            if label is not 'O':\n",
    "                preds.append(pred)\n",
    "                labels.append(label)\n",
    "    return labels, preds\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    '''Only compute metrics on homograph pronunciation predictions'''\n",
    "    preds_list, labels_list = align_predictions(p.predictions, p.label_ids)\n",
    "    targets, preds = homograph_check(preds_list, labels_list)\n",
    "#     preds = [tag2id[pred] for pred in preds]\n",
    "#     targets = [tag2id[target] for target in targets]\n",
    "#     preds = torch.as_tensor(preds)\n",
    "#     targets = torch.as_tensor(targets)\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(targets, preds),\n",
    "        \"balanced_accuracy_score\": balanced_accuracy_score(targets, preds),\n",
    "        #\"micro_auc_roc\": roc_auc_score(targets, preds, average='micro')\n",
    "#         \"micro_accuracy\": accuracy(preds, targets, class_reduction='micro', num_classes=len(unique_tags)),\n",
    "#         \"macro_accuracy\": accuracy(preds, targets, class_reduction='macro', num_classes=len(unique_tags)),\n",
    "    }\n",
    "\n",
    "def write_predictions_to_file(writer: TextIO, test_input_reader: TextIO, preds_list: List):\n",
    "    example_id = 0\n",
    "    for line in test_input_reader:\n",
    "        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "            writer.write(line)\n",
    "            if not preds_list[example_id]:\n",
    "                example_id += 1\n",
    "        elif preds_list[example_id]:\n",
    "            output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "            writer.write(output_line)\n",
    "        else:\n",
    "            logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique labels, enumeration/IDs, and make mappings\n",
    "unique_tags = [label.strip(\"\\n\") for label in open(LABELS).readlines()]\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make separate lists of aligned texts and tags\n",
    "train_texts, train_tags = read_set(TRAIN_TXT)\n",
    "dev_texts, dev_tags = read_set(DEV_TXT)\n",
    "test_texts, test_tags = read_set(TEST_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make encodings for text \n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME,add_prefix_space=True)\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "dev_encodings = tokenizer(dev_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NumPy boolean array indexing assignment cannot assign 12 input values to the 0 output values where the mask is true",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-52ff8381ca78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Make encodings for labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdev_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f856b03c7047>\u001b[0m in \u001b[0;36mencode_tags\u001b[1;34m(tags, encodings)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# set labels whose first offset position is 0 and the second is not 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mdoc_enc_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_offset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marr_offset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mencoded_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_enc_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: NumPy boolean array indexing assignment cannot assign 12 input values to the 0 output values where the mask is true"
     ]
    }
   ],
   "source": [
    "#Make encodings for labels\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "dev_labels = encode_tags(dev_tags, dev_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2f406bfe73c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"offset_mapping\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# don't want to pass this to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdev_encodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"offset_mapping\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWHDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdev_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWHDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_encodings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(dev_dataset[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "#Make datasets\n",
    "train_encodings.pop(\"offset_mapping\") # don't want to pass this to the model\n",
    "dev_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WHDataset(train_encodings, train_labels)\n",
    "dev_dataset = WHDataset(dev_encodings, dev_labels)\n",
    "#print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5ac4479d4d4ba48b32474ec005e2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c14cd29f73c4383839669e478a16ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=501200538.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b12822cb5633>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m model = RobertaForTokenClassification.from_pretrained(MODEL_NAME,\n\u001b[1;32m----> 8\u001b[1;33m                                                    num_labels=len(unique_tags))\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m                     \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m                     \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m                 )\n\u001b[0;32m    927\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         )\n\u001b[0;32m    952\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found in cache or force_download set to True, downloading to %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m             \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storing %s in cache at %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[1;32m-> 1022\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m                 if (\n\u001b[0;32m    520\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "model = RobertaForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                   num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-stratified-roberta-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_1,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "predictions, label_ids, metrics = trainer.predict(dev_dataset)\n",
    "preds_list, targets_list = align_predictions(predictions, label_ids)\n",
    "targets_list, preds_list = homograph_check(preds_list, targets_list)\n",
    "\n",
    "output_results_file = os.path.join(training_args.output_dir, \"results.txt\")\n",
    "if trainer.is_world_master():\n",
    "    with open(output_results_file, \"w\", encoding=\"utf8\") as writer:\n",
    "        for key, value in metrics.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "# Save predictions\n",
    "#output_predictions_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "to_df = zip(preds_list, targets_list)\n",
    "print('Micro Accuracy')\n",
    "df = pd.DataFrame(to_df, columns=['Preds', 'Targets'])\n",
    "df_correct = df[df['Preds'] == df['Targets']]\n",
    "print(len(df_correct)/len(df))\n",
    "print('Macro Accuracy')\n",
    "group_accuracies_for_mean = []\n",
    "group_accuracies = []\n",
    "for idx, group in df.groupby('Targets'):\n",
    "    grp = {}\n",
    "    grp[\"wordid\"] = idx\n",
    "    print(idx)\n",
    "    correct = group[group['Preds'] == group['Targets']]\n",
    "    accuracy = len(correct)/len(group)\n",
    "    print(accuracy)\n",
    "    grp[\"accuracy\"] = accuracy\n",
    "    group_accuracies.append(grp)\n",
    "    group_accuracies_for_mean.append(accuracy)\n",
    "print(mean(group_accuracies_for_mean))\n",
    "accuracies_df = pd.DataFrame(group_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions_file = os.path.join('./whd-variant-stratified-roberta-model', \"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df.sort_values(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids = pd.read_csv(\"C:/Users/jseal/Dev/dissertation/Data_Exploration/WHD/hids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc.sort_values([\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = hids_acc[hids_acc['accuracy'] < 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(15,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "homs = hids_acc[hids_acc[\"percent_ttl\"] < 0.2]['homograph'].tolist()\n",
    "homs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(homs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc['homograph'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and eval together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids = pd.read_csv(\"C:/Users/jseal/Dev/dissertation/Data_Exploration/WHD/train_eval_hids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "hids_acc = hids_acc[hids_acc['accuracy'] < 1.0]\n",
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(20,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hids_acc = pd.merge(hids, accuracies_df, on='wordid')\n",
    "hids_acc = hids_acc[hids_acc['percent_ttl'] < 0.1]\n",
    "hids_acc_prevalence = hids_acc[[\"wordid\", \"percent_ttl\"]]\n",
    "hids_acc_acc = hids_acc[[\"wordid\", \"accuracy\"]]\n",
    "ax = hids_acc_acc.plot(\n",
    "    x='wordid', linestyle='-', marker='o', color=\"red\")\n",
    "hids_acc_prevalence.plot(x='wordid', kind='bar', ax=ax, figsize=(20,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH SEED_2\n",
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "# config = DistilBertConfig.from_pretrained(MODEL_NAME, \n",
    "#                                           output_hidden_states=True, \n",
    "#                                           #idx2label=id2tag, \n",
    "#                                           num_labels=len(unique_tags))\n",
    "# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\n",
    "model = RoBertaForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                         num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-stratified-roberta-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_2,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH SEED_3\n",
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "# config = DistilBertConfig.from_pretrained(MODEL_NAME, \n",
    "#                                           output_hidden_states=True, \n",
    "#                                           #idx2label=id2tag, \n",
    "#                                           num_labels=len(unique_tags))\n",
    "# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\n",
    "model = RoBertaForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                         num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-stratified-roberta-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_3,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH SEED_4\n",
    "#Instantiate model, training args and trainer; train and evaluate\n",
    "# config = DistilBertConfig.from_pretrained(MODEL_NAME, \n",
    "#                                           output_hidden_states=True, \n",
    "#                                           #idx2label=id2tag, \n",
    "#                                           num_labels=len(unique_tags))\n",
    "# model = DistilBertForTokenClassification.from_pretrained(MODEL_NAME,config=config)\n",
    "model = RoBertaForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                         num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    load_best_model_at_end=True,\n",
    "    output_dir='./whd-variant-stratified-roberta-model',    # output directory\n",
    "    num_train_epochs=NUM_EPOCHS,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                # strength of weight decay\n",
    "    logging_dir=LOGS,                         # directory for storing logs\n",
    "    evaluation_strategy=STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    seed=SEED_4,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_huggingface",
   "language": "python",
   "name": "dissertation_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
