{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "NXT_SWBD = '/Users/jen/Dev/Dissertation/Data/SWBD/nxt_1.4.4'\n",
    "PHONWORDS = os.path.join(NXT_SWBD, 'xml/phonwords')\n",
    "WORDS = os.path.join(NXT_SWBD, 'xml/terminals')\n",
    "WHD = 'Dev/dissertation/Data/WikipediaHomographData/data'\n",
    "WHD_CTS = '~/Dev/Dissertation/Data/WHD_CTS'\n",
    "#NXT SWBD XML-specific variables\n",
    "PW = 'phonword'\n",
    "WD ='word'\n",
    "ORTH = 'orth'\n",
    "#Data\n",
    "whd_df = pd.read_csv(os.path.join(WHD,'WikipediaHomographData.csv'))\n",
    "nxt_whd_cts_df = whd_df.drop_duplicates(subset='homograph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orth_list(PATH : str, item_type : str) -> List: \n",
    "    orths = []\n",
    "    for f in glob.glob(os.path.join(PATH, '*')): \n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "        for item in root.findall(item_type): \n",
    "            orths.append(item.get(ORTH))\n",
    "    return orths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whd_subset(orths : list, item_type : str, nxt_whd_cts_df : pd.DataFrame) -> pd.DataFrame:\n",
    "    orths_ctr = Counter(orths)\n",
    "    nxt_whd_cts_df['{}_cts'.format(item_type)] = nxt_whd_cts_df['homograph'].apply(lambda hg : orths_ctr[hg])\n",
    "    nxt_whd_cts_df = nxt_whd_cts_df[['homograph', '{}_cts'.format(item_type)]]\n",
    "    return nxt_whd_cts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      35.802469\n",
      "1      14.814815\n",
      "4       6.172840\n",
      "2       4.938272\n",
      "8       3.703704\n",
      "9       3.703704\n",
      "5       3.086420\n",
      "6       2.469136\n",
      "15      1.851852\n",
      "21      1.851852\n",
      "7       1.851852\n",
      "3       1.851852\n",
      "11      1.234568\n",
      "56      1.234568\n",
      "24      0.617284\n",
      "12      0.617284\n",
      "14      0.617284\n",
      "16      0.617284\n",
      "18      0.617284\n",
      "20      0.617284\n",
      "495     0.617284\n",
      "533     0.617284\n",
      "29      0.617284\n",
      "483     0.617284\n",
      "444     0.617284\n",
      "147     0.617284\n",
      "130     0.617284\n",
      "370     0.617284\n",
      "110     0.617284\n",
      "98      0.617284\n",
      "57      0.617284\n",
      "52      0.617284\n",
      "48      0.617284\n",
      "43      0.617284\n",
      "41      0.617284\n",
      "39      0.617284\n",
      "38      0.617284\n",
      "31      0.617284\n",
      "25      0.617284\n",
      "Name: phonword_cts, dtype: float64\n",
      "0      36.419753\n",
      "1      14.197531\n",
      "4       6.172840\n",
      "2       4.938272\n",
      "5       3.703704\n",
      "8       3.703704\n",
      "6       2.469136\n",
      "7       2.469136\n",
      "11      1.851852\n",
      "15      1.851852\n",
      "9       1.851852\n",
      "3       1.851852\n",
      "21      1.851852\n",
      "44      1.234568\n",
      "23      0.617284\n",
      "13      0.617284\n",
      "14      0.617284\n",
      "16      0.617284\n",
      "18      0.617284\n",
      "20      0.617284\n",
      "491     0.617284\n",
      "487     0.617284\n",
      "30      0.617284\n",
      "441     0.617284\n",
      "148     0.617284\n",
      "131     0.617284\n",
      "374     0.617284\n",
      "110     0.617284\n",
      "100     0.617284\n",
      "59      0.617284\n",
      "56      0.617284\n",
      "54      0.617284\n",
      "50      0.617284\n",
      "41      0.617284\n",
      "40      0.617284\n",
      "39      0.617284\n",
      "545     0.617284\n",
      "31      0.617284\n",
      "25      0.617284\n",
      "Name: word_cts, dtype: float64\n",
      "        homograph  word_cts  phonword_cts\n",
      "0        abstract         0             0\n",
      "3          addict         0             0\n",
      "8       aggregate         0             0\n",
      "10       analyses         0             0\n",
      "11        animate         0             0\n",
      "13    approximate         0             0\n",
      "14     articulate         0             0\n",
      "17         august         0             0\n",
      "21        bologna         0             0\n",
      "23         buffet         0             0\n",
      "24         celtic         0             0\n",
      "30        conduct         0             0\n",
      "31       confines         0             0\n",
      "33   conglomerate         0             0\n",
      "34      conjugate         0             0\n",
      "35      conscript         0             0\n",
      "36        console         0             0\n",
      "37        consort         0             0\n",
      "39     consummate         0             0\n",
      "44       converse         0             0\n",
      "51     degenerate         0             0\n",
      "56      diagnoses         0             0\n",
      "57        diffuse         0             0\n",
      "58        discard         0             0\n",
      "59      discharge         0             0\n",
      "66         escort         0             0\n",
      "69     expatriate         0             0\n",
      "70        exploit         0             0\n",
      "71         export         0             0\n",
      "73        extract         0             0\n",
      "74       fragment         0             0\n",
      "79        implant         0             0\n",
      "82        incense         0             0\n",
      "85      increment         0             0\n",
      "87         insert         0             0\n",
      "89         insult         0             0\n",
      "92       intrigue         0             0\n",
      "94         invert         0             0\n",
      "97          jesus         0             0\n",
      "99       laminate         0             0\n",
      "109         moped         0             0\n",
      "111        nestle         0             0\n",
      "113      ornament         0             0\n",
      "114     overthrow         0             0\n",
      "115         pasty         0             0\n",
      "119       pervert         0             0\n",
      "120       pigment         0             0\n",
      "121        polish         0             0\n",
      "122     postulate         0             0\n",
      "123   precipitate         0             0\n",
      "124     predicate         0             0\n",
      "130         ravel         0             0\n",
      "135       recount         0             0\n",
      "139     rerelease         0             0\n",
      "141        retard         0             0\n",
      "142         rodeo         0             0\n",
      "151     syndicate         0             0\n",
      "153     transform         0             0\n",
      "(58, 3)\n",
      "35.80246913580247\n",
      "       homograph  word_cts  phonword_cts\n",
      "5         affect         9             9\n",
      "126      produce         9             9\n",
      "152         tear         6             9\n",
      "9      alternate         8             9\n",
      "110        mouth        11             9\n",
      "137       refuse         9             9\n",
      "160        winds         8             8\n",
      "53    deliberate         8             8\n",
      "112       object         8             8\n",
      "40       content         8             8\n",
      "127     progress         8             8\n",
      "95        invite         7             8\n",
      "88    instrument         7             7\n",
      "4       advocate         7             7\n",
      "107       mobile         7             7\n",
      "60      discount         6             6\n",
      "118       permit         6             6\n",
      "64     elaborate         6             6\n",
      "26       combine         5             6\n",
      "41       contest         5             5\n",
      "49      decrease         5             5\n",
      "27       compact         5             5\n",
      "138       reject         5             5\n",
      "144         sake         5             5\n",
      "133        rebel         4             4\n",
      "136       refund         4             4\n",
      "140       resume         4             4\n",
      "96       isolate         4             4\n",
      "149   supplement         4             4\n",
      "154   transplant         4             4\n",
      "129      protest         4             4\n",
      "75      frequent         4             4\n",
      "20       blessed         4             4\n",
      "22           bow         4             4\n",
      "7           aged         3             3\n",
      "15     associate         3             3\n",
      "106       misuse         3             3\n",
      "67      estimate         2             2\n",
      "61      document         2             2\n",
      "28      compound         2             2\n",
      "43      contrast         2             2\n",
      "50        defect         2             2\n",
      "91      intimate         2             2\n",
      "80     implement         2             2\n",
      "86      initiate         2             2\n",
      "93       invalid         1             1\n",
      "29      compress         1             1\n",
      "155    transport         1             1\n",
      "6      affiliate         1             1\n",
      "65      entrance         1             1\n",
      "148  subordinate         1             1\n",
      "146          sow         1             1\n",
      "16     attribute         1             1\n",
      "18          axes         1             1\n",
      "81        import         1             1\n",
      "83       incline         1             1\n",
      "38     construct         1             1\n",
      "62          dove         1             1\n",
      "63     duplicate         1             1\n",
      "47    coordinate         1             1\n",
      "48     correlate         1             1\n",
      "117      perfume         1             1\n",
      "72        expose         1             1\n",
      "52      delegate         1             1\n",
      "108     moderate         1             1\n",
      "55       deviate         1             1\n",
      "90   interchange         1             1\n",
      "104         mate         0             1\n",
      "2         abuses         1             1\n",
      "(69, 3)\n",
      "42.592592592592595\n",
      "       homograph  word_cts  phonword_cts\n",
      "158          use       545           533\n",
      "77         house       491           495\n",
      "132         read       487           483\n",
      "102         live       441           444\n",
      "98           job       374           370\n",
      "25         close       148           147\n",
      "131      reading       131           130\n",
      "103        lives       110           110\n",
      "147      subject       100            98\n",
      "159         wind        59            57\n",
      "68        excuse        44            56\n",
      "101      learned        56            56\n",
      "105       minute        54            52\n",
      "76      graduate        50            48\n",
      "128      project        44            43\n",
      "134       record        41            41\n",
      "145     separate        39            39\n",
      "156        upset        40            38\n",
      "125      present        31            31\n",
      "143          row        30            29\n",
      "116      perfect        25            25\n",
      "150      suspect        23            24\n",
      "1          abuse        21            21\n",
      "100         lead        21            21\n",
      "78        impact        21            21\n",
      "157         uses        20            20\n",
      "12   appropriate        18            18\n",
      "84      increase        16            16\n",
      "54        desert        15            15\n",
      "32      conflict        15            15\n",
      "161        wound        15            15\n",
      "46       convict        14            14\n",
      "42      contract        13            12\n",
      "45       convert        11            11\n",
      "19          bass        11            11\n",
      "(35, 3)\n",
      "21.604938271604937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Obtain lists of NXT SWBD graphemes\n",
    "phonwords = get_orth_list(PHONWORDS, PW)\n",
    "words = get_orth_list(WORDS, WD)\n",
    "#Get subsets of NXT SWBD graphemes also in Wikipedia Homograph Data with instance counts\n",
    "pw_cts_df = get_whd_subset(phonwords, PW, nxt_whd_cts_df)\n",
    "wd_cts_df = get_whd_subset(words, WD, nxt_whd_cts_df)\n",
    "#Serialize\n",
    "cts_df = pd.merge(wd_cts_df, pw_cts_df, on='homograph')\n",
    "DATETIME = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "cts_df.to_csv(os.path.join(WHD_CTS, 'nxt_whd_{}_cts.csv'.format(DATETIME)))\n",
    "#Review data\n",
    "#Ct percents (~35% Wikipedia graphemes have no instances in NXT SWBD; 14% have 1 instance)\n",
    "print(cts_df['phonword_cts'].value_counts(normalize=True) * 100)\n",
    "print(cts_df['word_cts'].value_counts(normalize=True) * 100)\n",
    "#Phonwords with 40 greatest count values\n",
    "cts_df.sort_values(by=['phonword_cts'], ascending=False).head(40)\n",
    "#Phonwords with 0 instances in NXT (58 graphmes, ~36% of WHs)\n",
    "zero_instances = cts_df[cts_df['phonword_cts'] == 0]\n",
    "print(zero_instances)\n",
    "print(zero_instances.shape)\n",
    "print(zero_instances.shape[0]/nxt_whd_cts_df.shape[0] * 100)\n",
    "#Phonwords with 1-9 instances in NXT (69 graphemes, ~43% of WHs)\n",
    "single_digit_instances = cts_df[(cts_df['phonword_cts'] > 0) & (cts_df['phonword_cts'] < 10)].sort_values(by=['phonword_cts'], ascending=False)\n",
    "print(single_digit_instances)\n",
    "print(single_digit_instances.shape)\n",
    "print(single_digit_instances.shape[0]/nxt_whd_cts_df.shape[0] *100)\n",
    "#Phonwords with 10 or more instances in NXT (35 graphemes, ~22% of WHs)\n",
    "more_instances = cts_df[cts_df['phonword_cts'] > 9].sort_values(by=['phonword_cts'], ascending=False)\n",
    "print(more_instances)\n",
    "print(more_instances.shape)\n",
    "print(more_instances.shape[0]/nxt_whd_cts_df.shape[0] *100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
