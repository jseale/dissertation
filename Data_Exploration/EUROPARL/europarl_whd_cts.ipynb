{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List, Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "EUROPARL = 'C:/Users/jseal/Dev/dissertation/Data/EUROPARL/'\n",
    "#ENG = os.path.join(EUROPARL, 'english')\n",
    "WHD = 'C:/Users/jseal/Dev/dissertation/Data/WikipediaHomographData/data/'\n",
    "WHD_CTS = 'C:/Users/jseal/Dev/dissertation/Data/WHD_CTS/'\n",
    "\n",
    "#Data\n",
    "whd_df = pd.read_csv(WHD + 'WikipediaHomographData.csv')\n",
    "europarl_whd_cts_df = whd_df.drop_duplicates(subset='homograph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(PATH : str) -> List: \n",
    "    tokens = []\n",
    "    nlp_file = nlp(' '.join(i), disable=['parser', 'tagger', 'ner']) \n",
    "    tokens = [token.lower_ for token in nlp_file if not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whd_subset(token_ctr : Counter, europarl_whd_cts_df : pd.DataFrame) -> pd.DataFrame:\n",
    "    europarl_whd_cts_df['cts'] = europarl_whd_cts_df['homograph'].apply(lambda hg : token_ctr[hg])\n",
    "    europarl_whd_cts_df = europarl_whd_cts_df[['homograph', 'cts']]\n",
    "    return europarl_whd_cts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-21-20 Need to check if these English texts are repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 483/483 [01:55<00:00,  4.19it/s]\n",
      "C:\\Users\\jseal\\anaconda3\\envs\\dissertation_huggingface\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ttl_cts = []\n",
    "os.chdir(EUROPARL)\n",
    "for f in glob.iglob('*.en'):\n",
    "    print(f)\n",
    "    en_file = open(f, 'r', encoding='UTF-8').read().split(' ')\n",
    "    n = 100000 #max allowed by spacy\n",
    "    x = [en_file[i:i + n] for i in range(0, len(en_file), n)]\n",
    "    tokens = []\n",
    "    for i in tqdm(x):\n",
    "        #Obtain lists of EUROPARL graphemes\n",
    "        tokens.append(get_tokens(\"./\" + f))\n",
    "    tokens =  [item for sublist in tokens for item in sublist]\n",
    "    token_ctr = Counter(tokens)\n",
    "    #Get subsets of Europarl graphemes also in Wikipedia Homograph Data with instance counts\n",
    "    cts_df = get_whd_subset(token_ctr, europarl_whd_cts_df)\n",
    "\n",
    "    #Serialize\n",
    "    DATETIME = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    \n",
    "    cts_df.to_csv(os.path.join(WHD_CTS, 'europarl_whd_{}_{}_cts.csv'.format(DATETIME, f[-8:-3])))\n",
    "\n",
    "    #ttl_cts.append(all_cts_df)     \n",
    "    \n",
    "#Serialize\n",
    "#all_cts_df = pd.concat(ttl_cts)\n",
    "DATETIME = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "#all_cts_df.to_csv(os.path.join(WHD_CTS, 'europarl_whd_{}_all_cts.csv'.format(DATETIME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     homograph  cts\n",
      "22     animate    0\n",
      "165    incense    0\n",
      "199   laminate    0\n",
      "224     nestle    0\n",
      "262      ravel    0\n",
      "280  rerelease    0\n",
      "286      rodeo    0\n",
      "(7, 2)\n",
      "4.320987654320987\n",
      "     homograph  cts\n",
      "246  postulate    9\n",
      "185   intrigue    8\n",
      "304  syndicate    8\n",
      "12   affiliate    7\n",
      "284     retard    7\n",
      "59    compress    5\n",
      "71   conscript    4\n",
      "171  increment    4\n",
      "242    pigment    4\n",
      "75     consort    3\n",
      "189     invert    3\n",
      "69   conjugate    2\n",
      "228   ornament    2\n",
      "(13, 2)\n",
      "8.024691358024691\n",
      "        homograph    cts\n",
      "318           use  34055\n",
      "155         house  29143\n",
      "252       present  21402\n",
      "256      progress  21014\n",
      "296       subject  20354\n",
      "312     transport  19509\n",
      "169      increase  17760\n",
      "157        impact  11848\n",
      "24    appropriate  11839\n",
      "201          lead   9867\n",
      "258       project   9446\n",
      "51          close   8200\n",
      "197           job   8053\n",
      "177    instrument   8006\n",
      "65       conflict   7822\n",
      "264       reading   7533\n",
      "161     implement   7149\n",
      "205          live   6816\n",
      "123      document   6587\n",
      "81        content   5733\n",
      "207         lives   5700\n",
      "254       produce   5360\n",
      "266          read   4225\n",
      "119     discharge   4088\n",
      "61        conduct   3805\n",
      "143        export   3580\n",
      "278        reject   3523\n",
      "10         affect   3323\n",
      "292      separate   2994\n",
      "2           abuse   2673\n",
      "270        record   2627\n",
      "244        polish   2039\n",
      "211        minute   2034\n",
      "290          sake   1788\n",
      "163        import   1713\n",
      "238        permit   1623\n",
      "95     coordinate   1568\n",
      "85       contract   1520\n",
      "276        refuse   1447\n",
      "191        invite   1408\n",
      "215        mobile   1405\n",
      "8        advocate   1366\n",
      "260       protest   1332\n",
      "4          abuses   1281\n",
      "34         august   1267\n",
      "137        excuse   1215\n",
      "234       perfect   1197\n",
      "203       learned   1193\n",
      "87       contrast   1167\n",
      "316          uses   1105\n",
      "141       exploit   1032\n",
      "226        object   1030\n",
      "222         mouth    887\n",
      "53        combine    870\n",
      "320          wind    865\n",
      "173      initiate    833\n",
      "20       analyses    825\n",
      "151      frequent    722\n",
      "218      moderate    702\n",
      "99       decrease    655\n",
      "302       suspect    651\n",
      "300    supplement    634\n",
      "282        resume    592\n",
      "107    deliberate    592\n",
      "135      estimate    510\n",
      "213        misuse    469\n",
      "308     transform    415\n",
      "77      construct    398\n",
      "179        insult    368\n",
      "14           aged    358\n",
      "30      associate    344\n",
      "314         upset    318\n",
      "91        convert    283\n",
      "129     elaborate    274\n",
      "193       isolate    271\n",
      "298   subordinate    270\n",
      "288           row    250\n",
      "0        abstract    246\n",
      "145        expose    237\n",
      "268         rebel    231\n",
      "105      delegate    201\n",
      "43        bologna    196\n",
      "83        contest    193\n",
      "175        insert    183\n",
      "109        desert    173\n",
      "310    transplant    161\n",
      "274        refund    161\n",
      "232         pasty    159\n",
      "147       extract    152\n",
      "63       confines    148\n",
      "32      attribute    146\n",
      "57       compound    143\n",
      "111       deviate    130\n",
      "45            bow    125\n",
      "324         wound    123\n",
      "101        defect    122\n",
      "127     duplicate    120\n",
      "230     overthrow    119\n",
      "37           axes    116\n",
      "306          tear    116\n",
      "322         winds    114\n",
      "187       invalid    106\n",
      "26    approximate    102\n",
      "103    degenerate    102\n",
      "117       discard     95\n",
      "131      entrance     90\n",
      "183      intimate     78\n",
      "16      aggregate     70\n",
      "294           sow     67\n",
      "28     articulate     63\n",
      "115       diffuse     54\n",
      "55        compact     52\n",
      "153      graduate     52\n",
      "181   interchange     52\n",
      "121      discount     51\n",
      "41        blessed     50\n",
      "195         jesus     48\n",
      "248   precipitate     44\n",
      "49         celtic     44\n",
      "149      fragment     37\n",
      "18      alternate     37\n",
      "113     diagnoses     31\n",
      "159       implant     31\n",
      "133        escort     28\n",
      "209          mate     27\n",
      "93        convict     26\n",
      "272       recount     24\n",
      "139    expatriate     24\n",
      "67   conglomerate     19\n",
      "47         buffet     18\n",
      "97      correlate     18\n",
      "6          addict     18\n",
      "167       incline     17\n",
      "39           bass     15\n",
      "73        console     14\n",
      "220         moped     14\n",
      "79     consummate     13\n",
      "89       converse     13\n",
      "236       perfume     12\n",
      "250     predicate     11\n",
      "240       pervert     10\n",
      "125          dove     10\n",
      "(142, 2)\n",
      "87.65432098765432\n"
     ]
    }
   ],
   "source": [
    "#Review data\n",
    "#cts_df = all_cts_df\n",
    "#Ct percents (~% Wikipedia graphemes have no instances in NXT SWBD; % have 1 instance)\n",
    "#print(cts_df['cts'].value_counts(normalize=True) * 100)\n",
    "\n",
    "#Tokens with 40 greatest count values\n",
    "cts_df.sort_values(by=['cts'], ascending=False).head(40)\n",
    "\n",
    "#Tokens with 0 instances  (graphemes, ~% of WHs)\n",
    "zero_instances = cts_df[cts_df['cts'] == 0]\n",
    "print(zero_instances)\n",
    "print(zero_instances.shape)\n",
    "print(zero_instances.shape[0]/europarl_whd_cts_df.shape[0] * 100)\n",
    "#Tokens with 1-9 instances (graphemes, ~% of WHs)\n",
    "single_digit_instances = cts_df[(cts_df['cts'] > 0) & (cts_df['cts'] < 10)].sort_values(by=['cts'], ascending=False)\n",
    "print(single_digit_instances)\n",
    "print(single_digit_instances.shape)\n",
    "print(single_digit_instances.shape[0]/europarl_whd_cts_df.shape[0] *100)\n",
    "#Tokens with 10 or more instances (graphemes, ~% of WHs)\n",
    "more_instances = cts_df[cts_df['cts'] > 9].sort_values(by=['cts'], ascending=False)\n",
    "print(more_instances)\n",
    "print(more_instances.shape)\n",
    "print(more_instances.shape[0]/europarl_whd_cts_df.shape[0] *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-level alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import AlignedSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nltk.align (from versions: none)\n",
      "ERROR: No matching distribution found for nltk.align\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk.align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.iglob('*.en'):\n",
    "    en_lines = open(f, 'r', encoding='UTF-8').readlines()\n",
    "\n",
    "for f in glob.iglob('*.fr'): \n",
    "    fr_lines = open(f, 'r', encoding=\"UTF-8\").readlines()\n",
    "\n",
    "en_nlp_lines = []\n",
    "for e in en_lines[:5]:\n",
    "    nlp_line = [token.text for token in nlp(e) if token.text != '\\n']\n",
    "    en_nlp_lines.append(nlp_line)    \n",
    "\n",
    "fr_nlp_lines = []\n",
    "for e in fr_lines[:5]:\n",
    "    nlp_line = [token.text for token in nlp(e) if token.text != '\\n']\n",
    "    fr_nlp_lines.append(nlp_line)    \n",
    "\n",
    "fren_lines = zip(en_nlp_lines, fr_nlp_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in fren_lines: \n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n",
      "['Reprise', 'de', 'la', 'session']\n",
      "\n",
      "['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "['Je', 'déclare', 'reprise', 'la', 'session', 'du', 'Parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', '17', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances', '.']\n",
      "\n",
      "['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'\", 'millennium', 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "['Comme', 'vous', 'avez', 'pu', 'le', 'constater', ',', 'le', 'grand', '\"', 'bogue', 'de', \"l'an\", '2000', '\"', 'ne', \"s'est\", 'pas', 'produit', '.', 'En', 'revanche', ',', 'les', 'citoyens', \"d'un\", 'certain', 'nombre', 'de', 'nos', 'pays', 'ont', 'été', 'victimes', 'de', 'catastrophes', 'naturelles', 'qui', 'ont', 'vraiment', 'été', 'terribles', '.']\n",
      "\n",
      "['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part', '-', 'session', '.']\n",
      "['Vous', 'avez', 'souhaité', 'un', 'débat', 'à', 'ce', 'sujet', 'dans', 'les', 'prochains', 'jours', ',', 'au', 'cours', 'de', 'cette', 'période', 'de', 'session', '.']\n",
      "\n",
      "['In', 'the', 'meantime', ',', 'I', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'Members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'European', 'Union', '.']\n",
      "['En', 'attendant', ',', 'je', 'souhaiterais', ',', 'comme', 'un', 'certain', 'nombre', 'de', 'collègues', 'me', \"l'ont\", 'demandé', ',', 'que', 'nous', 'observions', 'une', 'minute', 'de', 'silence', 'pour', 'toutes', 'les', 'victimes', ',', 'des', 'tempêtes', 'notamment', ',', 'dans', 'les', 'différents', 'pays', 'de', \"l'Union\", 'européenne', 'qui', 'ont', 'été', 'touchés', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in fren_lines:\n",
    "    algnsent = AlignedSent(e[0], e[1])\n",
    "    print(algnsent.words)\n",
    "    print(algnsent.mots)\n",
    "    print(algnsent.alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_huggingface",
   "language": "python",
   "name": "dissertation_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
