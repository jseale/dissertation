{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List, Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "EUROPARL = '~/Dev/dissertation/Data/EUROPARLv3'\n",
    "ENG = os.path.join(EUROPARL, 'english')\n",
    "WHD = '~/Dev/dissertation/Data/WikipediaHomographData/data'\n",
    "WHD_CTS = '~/Dev/dissertation/Data/WHD_CTS'\n",
    "\n",
    "#Data\n",
    "whd_df = pd.read_csv(os.path.join(WHD,'WikipediaHomographData.csv'))\n",
    "europarl_whd_cts_df = whd_df.drop_duplicates(subset='homograph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(PATH : str) -> List: \n",
    "    tokens = []\n",
    "    nlp_file = nlp(' '.join(i), disable=['parser', 'tagger', 'ner']) \n",
    "    tokens = [token.lower_ for token in nlp_file if not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whd_subset(token_ctr : Counter, europarl_whd_cts_df : pd.DataFrame) -> pd.DataFrame:\n",
    "    europarl_whd_cts_df['cts'] = europarl_whd_cts_df['homograph'].apply(lambda hg : token_ctr[hg])\n",
    "    europarl_whd_cts_df = europarl_whd_cts_df[['homograph', 'cts']]\n",
    "    return europarl_whd_cts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-21-20 Need to check if these English texts are repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_cts = []\n",
    "for f in glob.glob(os.path.join(ENG, '*')):\n",
    "    en_file = open(f, 'r', encoding='UTF-8').read().split(' ')\n",
    "    n = 100000 #max allowed by spacy\n",
    "    x = [en_file[i:i + n] for i in range(0, len(en_file), n)]\n",
    "    tokens = []\n",
    "    for i in tqdm(x):\n",
    "        #Obtain lists of EUROPARL graphemes\n",
    "        tokens.append(get_tokens(ENG))\n",
    "    tokens =  [item for sublist in tokens for item in sublist]\n",
    "    token_ctr = Counter(tokens)\n",
    "    #Get subsets of Europarl graphemes also in Wikipedia Homograph Data with instance counts\n",
    "    cts_df = get_whd_subset(token_ctr, europarl_whd_cts_df)\n",
    "\n",
    "    #Serialize\n",
    "    DATETIME = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    \n",
    "    cts_df.to_csv(os.path.join(WHD_CTS, 'europarl_whd_{}_{}_cts.csv'.format(DATETIME, f[-8:-3])))\n",
    "\n",
    "    #ttl_cts.append(all_cts_df)     \n",
    "    \n",
    "#Serialize\n",
    "#ttl_cts_df = pd.concat(ttl_cts)\n",
    "#DATETIME = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "#all_cts_df.to_csv(os.path.join(WHD_CTS, 'europarl_whd_{}_all_cts.csv'.format(DATETIME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Review data\n",
    "# cts_df = all_cts_df\n",
    "# #Ct percents (~% Wikipedia graphemes have no instances in NXT SWBD; % have 1 instance)\n",
    "# print(cts_df['cts'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# #Tokens with 40 greatest count values\n",
    "# all_cts_df.sort_values(by=['cts'], ascending=False).head(40)\n",
    "\n",
    "# #Tokens with 0 instances  (graphemes, ~% of WHs)\n",
    "# zero_instances = cts_df[cts_df['cts'] == 0]\n",
    "# print(zero_instances)\n",
    "# print(zero_instances.shape)\n",
    "# print(zero_instances.shape[0]/europarl_whd_cts_df.shape[0] * 100)\n",
    "# #Tokens with 1-9 instances (graphemes, ~% of WHs)\n",
    "# single_digit_instances = cts_df[(cts_df['cts'] > 0) & (cts_df['cts'] < 10)].sort_values(by=['cts'], ascending=False)\n",
    "# print(single_digit_instances)\n",
    "# print(single_digit_instances.shape)\n",
    "# print(single_digit_instances.shape[0]/europarl_whd_cts_df.shape[0] *100)\n",
    "# #Tokens with 10 or more instances (graphemes, ~% of WHs)\n",
    "# more_instances = cts_df[cts_df['cts'] > 9].sort_values(by=['cts'], ascending=False)\n",
    "# print(more_instances)\n",
    "# print(more_instances.shape)\n",
    "# print(more_instances.shape[0]/europarl_whd_cts_df.shape[0] *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "dissertation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
